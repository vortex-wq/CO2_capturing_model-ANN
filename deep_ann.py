# -*- coding: utf-8 -*-
"""DEEP_ANN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fobK8lWy0_PjhV0O2VhextzXLJFHy0gw
"""

# ============================================================
# Hybrid Deep ANN for CO₂ Absorption Modeling
# (Long Training ~30 minutes, robust ensemble)
# ============================================================

!pip install tensorflow pandas scikit-learn matplotlib tqdm

import pandas as pd, numpy as np, tensorflow as tf, time
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.initializers import HeNormal
from tensorflow.keras.optimizers.schedules import ExponentialDecay
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import KFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
from tqdm.notebook import tqdm
import matplotlib.pyplot as plt
from google.colab import files

# --- Upload your dataset ---
uploaded = files.upload()
filename = list(uploaded.keys())[0]
data = pd.read_csv(filename)
print("✅ Loaded:", filename)
print(data.head())

# --- Feature engineering ---
data['T_squared'] = data['Temperature_K'] ** 2
data['alpha_T'] = data['Alpha_mol_CO2_per_mol_amine'] * data['Temperature_K']
data['blend_alpha'] = data['Blending_ratio_wt_percent'] * data['Alpha_mol_CO2_per_mol_amine']

X = data[['Temperature_K','Blending_ratio_wt_percent','Alpha_mol_CO2_per_mol_amine',
          'T_squared','alpha_T','blend_alpha']].values
y = data['PCO2_kPa'].values.reshape(-1,1)   # choose 'qabs_kJ_per_mol_CO2' for qabs model

# --- Normalization ---
X_scaler = StandardScaler()
y_scaler = StandardScaler()
X_scaled = X_scaler.fit_transform(X)
y_scaled = y_scaler.fit_transform(y)

# --- Build deep model ---
def build_deep_ann(input_dim):
    model = Sequential([
        Dense(256, activation='relu', kernel_initializer=HeNormal(), input_shape=(input_dim,)),
        BatchNormalization(),
        Dropout(0.3),
        Dense(128, activation='relu'),
        BatchNormalization(),
        Dropout(0.3),
        Dense(64, activation='relu'),
        Dense(32, activation='relu'),
        Dense(1)
    ])
    lr_schedule = ExponentialDecay(initial_learning_rate=0.001, decay_steps=1000, decay_rate=0.96)
    model.compile(optimizer=Adam(learning_rate=lr_schedule), loss='mse', metrics=['mse'])
    return model

# --- 5-Fold Cross-Validation Ensemble ---
kf = KFold(n_splits=5, shuffle=True, random_state=42)
preds_all = np.zeros_like(y_scaled)
histories = []

start_time = time.time()
for fold, (train_idx, test_idx) in enumerate(kf.split(X_scaled)):
    print(f"\n Training fold {fold+1}/5")
    X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]
    y_train, y_test = y_scaled[train_idx], y_scaled[test_idx]

    model = build_deep_ann(X_train.shape[1])
    history = model.fit(
        X_train, y_train,
        validation_split=0.2,
        epochs=1000,
        batch_size=32,
        verbose=0,
        callbacks=[
            tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=60, restore_best_weights=True)
        ]
    )
    histories.append(history.history['val_loss'])

    preds_fold = model.predict(X_test)
    preds_all[test_idx] = preds_fold

elapsed = (time.time() - start_time)/60
print(f"\n⏱ Training completed in {elapsed:.1f} minutes.")

# --- Evaluate overall performance ---
y_true = y_scaler.inverse_transform(y_scaled)
y_pred = y_scaler.inverse_transform(preds_all)
mse = mean_squared_error(y_true, y_pred)
r2 = r2_score(y_true, y_pred)
smape = np.mean(100*np.abs(y_true - y_pred)/(np.abs(y_true)+np.abs(y_pred)+1e-6)/2)

print(f"\n Overall Metrics (5-Fold Ensemble)")
print(f"MSE: {mse:.3f}, R²: {r2:.3f}, SMAPE: {smape:.3f}%")

# --- Save predictions ---
pd.DataFrame({'Actual_PCO2': y_true.flatten(), 'Predicted_PCO2': y_pred.flatten()})\
  .to_csv("Deep_ANN_Ensemble_PCO2.csv", index=False)
files.download("Deep_ANN_Ensemble_PCO2.csv")

# --- Plot validation loss per fold ---
plt.figure(figsize=(7,4))
for i, val_loss in enumerate(histories):
    plt.plot(val_loss, label=f'Fold {i+1}')
plt.title('Validation Loss per Fold')
plt.xlabel('Epochs')
plt.ylabel('MSE')
plt.legend()
plt.grid(True)
plt.show()